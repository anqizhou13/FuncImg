{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First move all pickles files to a source directory for batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def copy_values_files(source_dir, destination_dir):\n",
    "    \"\"\"\n",
    "    Recursively search for all files named \"Values.pkl\" in subdirectories of source_dir\n",
    "    and copy them to the destination directory.\n",
    "    \n",
    "    Args:\n",
    "        source_dir (str): The directory to search for \"Values.pkl\" files.\n",
    "        destination_dir (str): The directory where the files will be copied.\n",
    "    \"\"\"\n",
    "    # Create the destination directory if it doesn't exist\n",
    "    if not os.path.exists(destination_dir):\n",
    "        os.makedirs(destination_dir)\n",
    "\n",
    "    # Walk through the directory tree\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        for file in files:\n",
    "            # Check if the file is named \"Values.pkl\"\n",
    "            if file == \"Values.pkl\":\n",
    "                # Get the full path of the file\n",
    "                file_path = os.path.join(root, file)\n",
    "                # get the subdirectory strings to make mirrored folders\n",
    "                file_name = os.path.normpath(file_path).split(os.path.sep)\n",
    "                file_folder = os.path.join(*file_name[4:-1])\n",
    "                \n",
    "                # Construct the final destination path\n",
    "                destination_path = os.path.join(destination_dir, file_folder, file)\n",
    "                \n",
    "                # make mirrored folders if they do not exist\n",
    "                Path(os.path.join(destination_dir, file_folder)).mkdir(parents=True,exist_ok=True)\n",
    "                # Copy the file to the destination directory\n",
    "                shutil.copyfile(file_path, destination_path)\n",
    "                print(f\"Copied {file_path} to {destination_path}\")\n",
    "\n",
    "# call function\n",
    "source_directory = \"/Volumes/TOSHIBA/IMAGING\"\n",
    "destination_directory = \"/Volumes/TOSHIBA/source_files\"\n",
    "copy_values_files(source_directory, destination_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursively open pkl files and save the data in a readable csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot open /Volumes/TOSHIBA/source_files/GCaMP_data/binary/R61D08_38H09LexA_LexAopGCaMP7s/set0/Values.pkl\n",
      "Cannot open /Volumes/TOSHIBA/source_files/GCaMP_data/binary/R61D08_UAS-Ab1-40_38H09LexA_LexAopGCaMP7s/set0/Values.pkl\n",
      "Cannot open /Volumes/TOSHIBA/source_files/GCaMP_data/binary/R61D08_UAS-Ab1-42_38H09LexA_LexAopGCaMP7s/set0/Values.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "\n",
    "source_dir = \"/Volumes/TOSHIBA/source_files\"\n",
    "\n",
    "for root, dirs, files in os.walk(source_dir):\n",
    "    for file in files:\n",
    "        # Check if the file is named \"Values.pkl\"\n",
    "        if file == \"Values.pkl\":\n",
    "            # Get the full path of the file\n",
    "            file_path = os.path.join(root,file)\n",
    "            # load the pickle format\n",
    "            try:\n",
    "                raw,dF,dF_f0,mean_baseline,mean_dF_f0,__ = pickle.load(open(file_path, 'rb'))\n",
    "            except:\n",
    "                print(f\"Cannot open {file_path}\")\n",
    "                continue\n",
    "\n",
    "            # also output the mean and sem vector\n",
    "            vars = [raw,dF,dF_f0]\n",
    "            varnames = ['rawF','dF','dF_F0']\n",
    "            varnames_mean = ['rawF_meanSEM','dF_meanSEM','dF_F0_meanSEM']\n",
    "\n",
    "            # for each metric\n",
    "            with pd.ExcelWriter('{}/summary_data.xlsx'.format(root)) as writer:\n",
    "                for j,var in enumerate(vars):\n",
    "                    # convert the variable to array\n",
    "                    var = np.array(var,dtype='object')\n",
    "                    df = pd.DataFrame(var)\n",
    "                    df.to_excel(writer, sheet_name = varnames[j])\n",
    "\n",
    "                    # then for each time point, compute the mean and sem over all larvae\n",
    "                    mean = []\n",
    "                    sem = []\n",
    "                    for t in range(len(var[0])-1):\n",
    "                        mean.append(np.mean([var[l][t] for l in range(len(var))]))\n",
    "                        sem.append(stats.sem([var[l][t] for l in range(len(var))]))\n",
    "                    df = pd.DataFrame({'mean':mean,'sem':sem})\n",
    "                    df.to_excel(writer, sheet_name = varnames_mean[j])\n",
    "\n",
    "                    summary = pd.Series([[np.mean(mean_baseline),stats.sem(mean_baseline)],[np.mean(mean_dF_f0),stats.sem(mean_dF_f0)]], index=['average','sem'])\n",
    "                    df = pd.DataFrame({'mean baseline F0 per trial':mean_baseline,'mean dF_F0 per trial':mean_dF_f0})\n",
    "                    df = pd.concat([df,summary])\n",
    "                    df.to_excel(writer, sheet_name = 'summary')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
